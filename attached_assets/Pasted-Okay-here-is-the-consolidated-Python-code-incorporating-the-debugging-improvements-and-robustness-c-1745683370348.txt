Okay, here is the consolidated Python code incorporating the debugging improvements and robustness checks we discussed.

Key Changes:

Enhanced Logging: More detailed logging, including raw API response and specific error types. Basic logging configuration added at the start.

API Key Check: Clear logging if the key is missing or found (masked).

API Error Handling: Logs the full response body on non-200 status codes.

Network Error Handling: Specific try...except block for requests.exceptions.RequestException.

General Exception Handling: Logs exception type and includes traceback for better debugging.

Raw Response Logging: Added a DEBUG log statement to show the exact content received from the API before cleaning.

Standardized Fallback: Consistently uses create_direct_output for API/translation failures, as it's designed to display the original articles.

Imports: Added traceback for detailed error logging.

import os
import logging
import json
import requests
import re
import html
import traceback # Added for detailed error logging
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup

# Configure logging (Add this block)
# Adjust level (DEBUG, INFO, WARNING, ERROR) and format as needed
logging.basicConfig(
    level=logging.INFO,  # Set to DEBUG for very verbose output
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Mistral AI API configuration
MISTRAL_API_KEY = os.environ.get("MISTRAL_API_KEY", "")

# --- Fallback Function (Original - Keep if used elsewhere, but not primary API fallback) ---
# Note: This function seems less suitable as a primary API failure fallback
# compared to create_direct_output which shows original articles.
# def create_fallback_output(articles: List[Dict[str, Any]], error_message: str) -> Dict[str, Any]:
#     """
#     Create a simple HTML fallback for displaying articles when API fails.
#     (Consider if create_direct_output is more appropriate for API failures)
#     """
#     # ... (original implementation of create_fallback_output) ...
#     # This function remains unchanged from your original code
#     soup = BeautifulSoup("<h1>Greek Domestic News Summary</h1>", 'html.parser')
#     error_p = soup.new_tag("p")
#     error_p["style"] = "color: #e74c3c; font-weight: bold;"
#     error_p.string = f"Unable to summarize news: {error_message}"
#     soup.append(error_p)
#     retry_p = soup.new_tag("p")
#     retry_p.string = "Here are the raw news articles we found:"
#     soup.append(retry_p)
#     limit = min(12, len(articles))
#     for i, article in enumerate(articles[:limit], 1):
#         h2 = soup.new_tag("h2")
#         h2.string = f"{i}. {article.get('title', 'Untitled Article')}"
#         soup.append(h2)
#         source_p = soup.new_tag("p")
#         source_p["class"] = "news-source"
#         source_p.string = f"Source: {article.get('source', 'Unknown Source')}"
#         soup.append(source_p)
#         url = article.get('url', '')
#         if url:
#             a = soup.new_tag("a")
#             a["href"] = url
#             a["target"] = "_blank"
#             a["class"] = "read-more"
#             a.string = "Read Full Article"
#             soup.append(a)
#     return {
#         "html_content": str(soup),
#         "article_count": len(articles),
#         "sources": list(set(article.get('source', '') for article in articles)),
#         "error": error_message
#     }

# --- HTML Cleaning Function ---
def clean_html_content(html_content: str) -> str:
    """
    Clean and process the HTML content from the API response.

    Args:
        html_content: Raw HTML content from the API

    Returns:
        Cleaned and properly formatted HTML content string
    """
    try:
        # Basic check if it looks like HTML
        is_likely_html = '<' in html_content and '>' in html_content
        if not is_likely_html:
             logger.warning("Response doesn't seem to contain HTML tags. Wrapping raw content.")
             clean_content = html.escape(html_content) # Escape any potential stray tags
             return f"<h1>Greek Domestic News Summary</h1><p>The content received from the AI was not in the expected HTML format. Displaying raw response:</p><pre>{clean_content}</pre>"

        # Try to find the start of the actual HTML content (e.g., starting with <h1>)
        html_start_index = -1
        potential_starts = ['<h1', '<div', '<p', '<h2']
        for tag_start in potential_starts:
            html_start_index = html_content.find(tag_start)
            if html_start_index != -1:
                break

        if html_start_index > 0:
            # Found a potential start tag after some introductory text
            logger.info(f"Detected potential introductory text before HTML. Extracting from index {html_start_index}.")
            html_content = html_content[html_start_index:]
        elif html_start_index == -1:
             logger.warning("Could not find standard starting HTML tags (h1, div, p, h2). Processing content as is.")
             # Proceed, maybe it's just fragments

        # Parse with BeautifulSoup to clean up and structure
        soup = BeautifulSoup(html_content, 'html.parser')

        # Ensure all links open in a new tab and have the correct class
        for a_tag in soup.find_all('a'):
            if a_tag.has_attr('href'):
                a_tag['target'] = '_blank'
                a_tag['class'] = 'read-more' # Ensure class consistency

        # Ensure there's a main title (H1)
        h1_tag = soup.find('h1')
        if not h1_tag:
            logger.warning("No H1 title found in the response. Adding a default title.")
            new_h1 = soup.new_tag('h1')
            new_h1.string = 'Greek Domestic News Summary'
            # Try inserting at the beginning, handling potential parse issues
            body_tag = soup.find('body')
            if body_tag:
                 body_tag.insert(0, new_h1)
            else:
                 # If no body, just prepend to the soup object (might result in slightly invalid structure but better than nothing)
                 soup.insert(0, new_h1)


        # Count stories for logging purposes
        h2_tags = soup.find_all('h2')
        logger.info(f"Found {len(h2_tags)} news items (H2 tags) in the cleaned HTML.")

        # Basic validation: Check if there are actual news items (h2 tags)
        if not h2_tags:
            logger.warning("Cleaned HTML does not contain any H2 tags (news items). Returning a message.")
            # Return a more informative message instead of potentially empty/malformed HTML
            fallback_soup = BeautifulSoup("<h1>Greek Domestic News Summary</h1>", 'html.parser')
            p_tag = fallback_soup.new_tag('p')
            p_tag.string = "The AI response did not contain formatted news items as expected."
            fallback_soup.append(p_tag)
            pre_tag = fallback_soup.new_tag('pre')
            # Show the original (before cleaning) content for debugging
            pre_tag.string = html.escape(html_content)
            fallback_soup.append(pre_tag)
            return str(fallback_soup)

        return str(soup)

    except Exception as e:
        logger.error(f"Error cleaning HTML content: {str(e)}", exc_info=True)
        # Return a safe fallback in case of unexpected cleaning errors
        return f"<h1>Greek Domestic News Summary</h1><p>There was an error processing the news content received from the AI. Details: {html.escape(str(e))}</p>"


# --- Direct Output Function (Fallback for API/Translation Failure) ---
def create_direct_output(news_data: List[Dict[str, Any]], error_message: Optional[str] = None) -> Dict[str, Any]:
    """
    Creates HTML output directly from raw news data when API/translation fails.

    Args:
        news_data: List of dictionaries containing news articles.
        error_message: Optional error message to display.

    Returns:
        Dictionary with HTML content, article count, sources, and direct mode flag.
    """
    logger.info(f"Creating direct output. Reason: {error_message or 'N/A'}")
    soup = BeautifulSoup("<h1>Greek Domestic News Summary</h1>", 'html.parser')

    note_p = soup.new_tag("p")
    note_p["style"] = "font-style: italic; margin-bottom: 15px;"
    note_p.string = "Displaying the latest raw news articles from Greek sources:"
    soup.append(note_p)

    if error_message:
        error_p = soup.new_tag("p")
        error_p["style"] = "color: #e74c3c; font-weight: bold; margin-bottom: 20px; border: 1px solid #e74c3c; padding: 10px; background-color: #fadedb;"
        error_p.string = f"Note: AI translation/summarization failed. ({error_message})"
        soup.append(error_p)

    limit = min(12, len(news_data)) # Show up to 12 articles directly
    logger.info(f"Displaying {limit} out of {len(news_data)} available articles directly.")

    for i, article in enumerate(news_data[:limit], 1):
        title = article.get('title', 'Untitled Article')
        h2 = soup.new_tag("h2")
        h2.string = f"{i}. {title}" # Original title
        soup.append(h2)

        content = article.get('content', '')
        if content:
            # Simple excerpt logic
            sentences = re.split(r'(?<=[.!?])\s+', content) # Split sentences
            excerpt = '. '.join(sentences[:2]).strip() # Take first 2 sentences
            if len(excerpt) < 50 and len(sentences) > 2: # If first 2 are very short, add 3rd
                 excerpt = '. '.join(sentences[:3]).strip()
            if not excerpt: # Fallback if sentence split failed
                 excerpt = content[:250].strip() + ("..." if len(content) > 250 else "")

            p = soup.new_tag("p")
            p.string = excerpt # Original content excerpt
            soup.append(p)

        source = article.get('source', 'Unknown Source')
        source_p = soup.new_tag("p")
        source_p["class"] = "news-source"
        source_p.string = f"Source: {source}"
        soup.append(source_p)

        url = article.get('url', '')
        if url:
            a = soup.new_tag("a", href=url, target="_blank", attrs={"class": "read-more"})
            a.string = "Read Full Article (Original Source)"
            soup.append(a)

    return {
        "html_content": str(soup),
        "article_count": len(news_data), # Total articles found
        "sources": list(set(article.get('source', '') for article in news_data if article.get('source'))),
        "direct_mode": True, # Flag indicating API was bypassed
        "error": error_message # Pass along the error reason
    }


# --- Mistral AI Summarization Function ---
def summarize_news(news_data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    Summarizes and translates Greek news using Mistral AI.

    Args:
        news_data: List of dictionaries containing news articles.

    Returns:
        Dictionary with summarized/translated HTML content or direct output on failure.
    """
    if not news_data:
         logger.warning("No news data provided to summarize.")
         return create_direct_output([], "No articles found to process.")

    if not MISTRAL_API_KEY:
        logger.error("MISTRAL_API_KEY environment variable not set or empty.")
        return create_direct_output(news_data, "Missing Mistral API Key configuration")
    else:
        # Log confirmation but mask the key for security
        logger.info(f"Mistral API Key found (ending with ...{MISTRAL_API_KEY[-4:]})")

    try:
        # System Prompt: Keep it concise and focused on the task and format.
        system_prompt = """
        You are an expert translator and news summarizer. Your task is to translate Greek news articles into English and format them as clean HTML.
        Follow the user's instructions precisely regarding the output format.
        Output *only* the HTML structure requested, starting with <h1> and ending with the last </a> tag. Do not include any introductory or concluding text outside the HTML tags.
        """

        # User Prompt: Prepare article data and give explicit formatting instructions.
        user_prompt = "Translate the following Greek news articles into English and create a summary HTML page.\n\n"

        max_articles_to_send = 20 # Limit input to API
        selected_articles = news_data[:min(max_articles_to_send, len(news_data))]
        logger.info(f"Preparing {len(selected_articles)} articles for the API request.")

        for i, article in enumerate(selected_articles, 1):
            user_prompt += f"--- ARTICLE {i} ---\n"
            user_prompt += f"TITLE_GR: {article.get('title', 'No Title Provided')}\n"
            user_prompt += f"SOURCE: {article.get('source', 'Unknown Source')}\n"
            user_prompt += f"URL: {article.get('url', 'No URL Provided')}\n"
            content = article.get('content', '')
            # Use a snippet for context, reduce token usage
            snippet = (content[:250].strip() + "...") if len(content) > 250 else content.strip()
            user_prompt += f"CONTENT_SNIPPET_GR: {snippet if snippet else '[No Content Snippet]'}\n\n"

        user_prompt += """
--- INSTRUCTIONS ---
1.  Translate the TITLE_GR of each article to English.
2.  Based on the TITLE_GR and CONTENT_SNIPPET_GR, write a concise 2-3 sentence summary in English for each article.
3.  Format the output as *pure HTML*, containing exactly 12 news story entries (use the first 12 articles provided if more than 12 were sent).
4.  Strictly adhere to this HTML structure for each of the 12 entries:

    <h2>[INCREMENTING_NUMBER]. [TRANSLATED_ENGLISH_TITLE]</h2>
    <p>[CONCISE_ENGLISH_SUMMARY]</p>
    <p class="news-source">Source: [ORIGINAL_SOURCE_NAME]</p>
    <a href="[EXACT_ORIGINAL_URL]" target="_blank" class="read-more">Read Full Article</a>

5.  Start the entire response *directly* with a single main heading: <h1>Greek Domestic News Summary</h1>
6.  Ensure all 12 entries follow the h2, p, p, a structure.
7.  Use the exact URL provided for each article in the href attribute.
8.  Do *not* add any text before the <h1> tag or after the final </a> tag.
9.  Double-check: The final output must be valid HTML containing exactly one <h1> and exactly twelve <h2> sections.
"""

        logger.debug("Sending request to Mistral API...")

        headers = {
            "Content-Type": "application/json",
            "Accept": "application/json", # Explicitly accept JSON
            "Authorization": f"Bearer {MISTRAL_API_KEY}"
        }

        payload = {
            "model": "mistral-large-latest", # Or your preferred model
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.3, # Slightly creative but mostly factual
            "max_tokens": 4096, # Allow ample space for 12 summaries + HTML
            "response_format": {"type": "text"} # Ensure text response format
        }

        api_url = "https://api.mistral.ai/v1/chat/completions"
        response = requests.post(api_url, headers=headers, json=payload, timeout=120) # Add timeout

        # --- Response Handling ---
        if response.status_code == 200:
            logger.info(f"Mistral API request successful (Status Code: {response.status_code})")
            response_data = response.json()

            if not response_data.get("choices") or not response_data["choices"][0].get("message"):
                 logger.error("API response seems malformed: 'choices' or 'message' missing.")
                 logger.debug(f"Malformed response data: {response_data}")
                 return create_direct_output(news_data, "API response structure was invalid.")

            summary_content = response_data["choices"][0]["message"]["content"]
            finish_reason = response_data["choices"][0].get("finish_reason", "unknown")
            logger.info(f"API call finished. Reason: {finish_reason}")
            if finish_reason == "length":
                logger.warning("API response may be truncated because finish reason was 'length'.")

            # Log the raw response *before* cleaning
            logger.debug(f"Raw API Response Content:\n----\n{summary_content}\n----")

            cleaned_summary_html = clean_html_content(summary_content)

            # Verify cleaned HTML structure (simple check)
            soup_check = BeautifulSoup(cleaned_summary_html, 'html.parser')
            story_count = len(soup_check.find_all('h2'))

            # Require a reasonable number of stories to consider it successful
            min_expected_stories = 5 # Be flexible, model might not hit exactly 12
            if story_count >= min_expected_stories:
                logger.info(f"Successfully processed API response. Found {story_count} stories in cleaned HTML.")
                return {
                    "html_content": cleaned_summary_html,
                    "article_count": story_count, # Report actual stories returned
                    "sources": list(set(article.get('source', '') for article in selected_articles if article.get('source'))),
                    "translated": True,
                    "error": None # Indicate success
                }
            else:
                logger.warning(f"API response parsing yielded only {story_count} stories (less than minimum {min_expected_stories}). Falling back to direct output.")
                return create_direct_output(news_data, f"API response format issue (found {story_count} stories)")

        else:
            # Handle API errors (non-200 status code)
            error_details = f"Status Code: {response.status_code}"
            try:
                # Try to get more details from the JSON response body
                error_body = response.json()
                error_details += f" - Body: {json.dumps(error_body)}"
            except json.JSONDecodeError:
                # If body is not JSON, log the raw text
                error_details += f" - Body: {response.text}"

            logger.error(f"Mistral API error: {error_details}")
            # Create specific error message for fallback
            fallback_msg = f"API Error {response.status_code}"
            if response.status_code == 401: fallback_msg += " (Check API Key)"
            if response.status_code == 429: fallback_msg += " (Rate Limit Reached)"
            if response.status_code >= 500: fallback_msg += " (Server Issue)"

            return create_direct_output(news_data, fallback_msg)

    # --- Exception Handling ---
    except requests.exceptions.Timeout:
        logger.error("Network error: Request to Mistral API timed out.")
        return create_direct_output(news_data, "Network Timeout connecting to AI service")
    except requests.exceptions.RequestException as e:
        logger.error(f"Network error connecting to Mistral API: {str(e)}")
        return create_direct_output(news_data, f"Network Error: {str(e)}")
    except json.JSONDecodeError as e:
         logger.error(f"Error decoding JSON response from API: {str(e)}")
         # Log the raw response text that caused the error if possible
         try:
              logger.error(f"Raw response text causing JSON error: {response.text}")
         except NameError: # response might not be defined if error happened earlier
              pass
         return create_direct_output(news_data, "Invalid JSON response from AI service")
    except Exception as e:
        # Catch any other unexpected errors during the process
        logger.error(f"Unexpected error during Mistral AI processing ({type(e).__name__}): {str(e)}")
        logger.error(traceback.format_exc()) # Log the full traceback for debugging
        return create_direct_output(news_data, f"Unexpected Error: {str(e)}")

# --- Example Usage (Optional) ---
if __name__ == '__main__':
    # This block runs only when the script is executed directly
    # Set environment variable MISTRAL_API_KEY before running for API test
    # export MISTRAL_API_KEY='YOUR_ACTUAL_API_KEY'

    print("Running Mistral AI News Summarizer Test...")
    logger.info("Starting example run...")

    # Example dummy news data (replace with your actual data loading)
    sample_news_data = [
        {
            "title": "Κύμα καύσωνα πλήττει την Ελλάδα",
            "source": "Kathimerini",
            "url": "http://example.com/kathimerini/heatwave",
            "content": "Υψηλές θερμοκρασίες αναμένονται τις επόμενες ημέρες σε όλη τη χώρα. Οι αρχές συνιστούν προσοχή στις ευπαθείς ομάδες. Το θερμόμετρο θα αγγίξει τους 40 βαθμούς Κελσίου."
        },
        {
            "title": "Νέες επενδύσεις στον τουρισμό",
            "source": "Naftemporiki",
            "url": "http://example.com/naftemporiki/tourism",
            "content": "Η κυβέρνηση ανακοίνωσε σχέδια για την προσέλκυση νέων επενδύσεων στον τουριστικό τομέα, με έμφαση στον ποιοτικό τουρισμό και τις υποδομές."
        },
        # Add more articles (up to 20 for the API call)
        {
            "title": "Πολιτική αντιπαράθεση στη Βουλή",
            "source": "Proto Thema",
            "url": "http://example.com/protothema/politics",
            "content": "Έντονη συζήτηση πραγματοποιήθηκε σήμερα στη Βουλή σχετικά με το νέο φορολογικό νομοσχέδιο. Η αντιπολίτευση άσκησε κριτική."
        }
        # ... add up to 12-20 articles total for a good test
    ] * 5 # Multiply to get more articles for testing limits


    print(f"\nAttempting to summarize {len(sample_news_data)} articles...")

    # Call the main function
    result = summarize_news(sample_news_data)

    print("\n--- Result ---")
    if result.get("error"):
        print(f"An error occurred: {result['error']}")
    if result.get("direct_mode"):
        print("API was bypassed. Displaying direct output.")
    if result.get("translated"):
        print("News successfully summarized and translated by AI.")

    print(f"Article Count Reported: {result.get('article_count')}")
    print(f"Sources: {result.get('sources')}")

    print("\n--- HTML Content ---")
    # Print only the first 1000 characters of HTML for brevity in console
    html_output = result.get("html_content", "No HTML content generated.")
    print(html_output[:1000] + ('...' if len(html_output) > 1000 else ''))

    # Optionally, save the full HTML to a file for inspection
    # with open("summary_output.html", "w", encoding="utf-8") as f:
    #     f.write(html_output)
    # print("\nFull HTML content saved to summary_output.html")

    logger.info("Example run finished.")


How to Use:

Save: Save the code as a Python file (e.g., news_summarizer.py).

API Key: Make sure you have set the MISTRAL_API_KEY environment variable before running the script.

Linux/macOS: export MISTRAL_API_KEY='your_actual_mistral_api_key'

Windows (cmd): set MISTRAL_API_KEY=your_actual_mistral_api_key

Windows (PowerShell): $env:MISTRAL_API_KEY='your_actual_mistral_api_key'

Install Dependencies:

pip install requests beautifulsoup4
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Run: Execute the script from your terminal:

python news_summarizer.py
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

Check Output & Logs: Observe the console output and any log messages. If errors occur, the logs (especially DEBUG level if enabled) should provide detailed information about where the process failed (API connection, API response error, content cleaning, etc.). The console output will indicate if it fell back to create_direct_output.

This version is much more robust and provides significantly better feedback when things go wrong, making it easier to diagnose API key issues, rate limits, network problems, or unexpected responses from the Mistral API.